{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ğŸ“„ PDF Translator - Full Gradio Version\n",
        "\n",
        "**Translate scientific PDFs with the complete Gradio UI**\n",
        "\n",
        "> ğŸ”— **GitHub:** [error-wtf/pdf-translator-enhanced](https://github.com/error-wtf/pdf-translator-enhanced)  \n",
        "> Based on [thelanguagenerd/pdf-translator](https://github.com/thelanguagenerd/pdf-translator)\n",
        "\n",
        "Â© 2025 Sven Kalinowski with small help of Lino Casu  \n",
        "Licensed under the Anti-Capitalist Software License v1.4\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ How It Works\n",
        "\n",
        "1. **Run all cells** - Installs Ollama, clones repo, installs dependencies\n",
        "2. **Gradio launches** with a **public share link**\n",
        "3. **Access from any device** - The link works for 72 hours\n",
        "4. **Full features** - All 20 languages, formula protection, table detection\n",
        "\n",
        "---\n",
        "\n",
        "## âš¡ Features\n",
        "\n",
        "- **ğŸ”¬ Formula Protection** - 60+ LaTeX patterns protected during translation\n",
        "- **ğŸŒ 20 Languages** - German, English, Arabic, Hebrew, Ukrainian, Japanese, Chinese, Hindi...\n",
        "- **ğŸ“Š Table Detection** - Preserves table structure\n",
        "- **ğŸ–¼ï¸ Caption Anchoring** - Figures stay with captions\n",
        "- **ğŸ”’ 100% Local** - With Ollama, no data leaves Google's servers\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¤– Recommended Models\n",
        "\n",
        "| Model | Size | Description | VRAM |\n",
        "|-------|------|-------------|------|\n",
        "| **`openchat:7b`** â­ | 4.1 GB | ChatGPT-like, great for translations | 8 GB |\n",
        "| `qwen2.5:7b` | 4.4 GB | Excellent multilingual | 8 GB |\n",
        "| `llama3.1:8b` | 4.7 GB | Meta's latest | 10 GB |\n",
        "| `neural-chat:7b` | 4.1 GB | Intel optimized | 8 GB |\n",
        "| `qwen2.5:14b` | 9 GB | Best quality | 16 GB |\n",
        "\n",
        "---\n",
        "\n",
        "## GPU Runtime Setup\n",
        "\n",
        "1. Click **Runtime** â†’ **Change runtime type**\n",
        "2. Select **T4 GPU** (free) or **A100** (Colab Pro)\n",
        "3. Click **Save**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "#@title 1. Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "import torch\n",
        "print(f\"\\nâœ… CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"ğŸ® GPU: {gpu_name}\")\n",
        "    print(f\"ğŸ’¾ VRAM: {vram_gb:.1f} GB\")\n",
        "    if vram_gb >= 24:\n",
        "        print(\"\\nğŸ“Œ Recommended: qwen2.5:14b or mistral-nemo:12b\")\n",
        "    elif vram_gb >= 16:\n",
        "        print(\"\\nğŸ“Œ Recommended: llama3.1:8b or qwen2.5:7b\")\n",
        "    elif vram_gb >= 8:\n",
        "        print(\"\\nğŸ“Œ Recommended: llama3.2:3b or phi3:mini\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ Low VRAM - use OpenAI backend or smaller models\")\n",
        "else:\n",
        "    print(\"\\nâŒ No GPU detected! Please enable GPU runtime:\")\n",
        "    print(\"   Runtime â†’ Change runtime type â†’ T4 GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 2. Install Ollama\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "print(\"\\nâœ… Ollama installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 3. Start Ollama & Download Model\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"ğŸš€ Starting Ollama server...\")\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if running\n",
        "try:\n",
        "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "    if r.status_code == 200:\n",
        "        print(\"âœ… Ollama server running!\")\n",
        "except:\n",
        "    print(\"âš ï¸ Ollama not responding. Run this cell again.\")\n",
        "\n",
        "# Download model\n",
        "# openchat:7b is a ChatGPT-like open source model, great for translations\n",
        "model = \"openchat:7b\"  #@param [\"openchat:7b\", \"qwen2.5:7b\", \"llama3.1:8b\", \"qwen2.5:14b\", \"mistral:7b\", \"mistral-nemo:12b\", \"llama3.2:3b\", \"neural-chat:7b\"]\n",
        "print(f\"\\nğŸ“¥ Downloading {model} (this may take a few minutes)...\")\n",
        "print(\"ğŸ’¡ openchat:7b is a ChatGPT-like open source model - great for translations!\")\n",
        "!ollama pull {model}\n",
        "print(f\"\\nâœ… Model {model} ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "#@title 4. Clone Repository & Install Dependencies\n",
        "%cd /content\n",
        "!rm -rf pdf-translator-enhanced 2>/dev/null\n",
        "!git clone https://github.com/error-wtf/pdf-translator-enhanced.git\n",
        "%cd pdf-translator-enhanced\n",
        "\n",
        "print(\"\\nğŸ“¦ Installing dependencies...\")\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"\\nâœ… All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_gradio"
      },
      "outputs": [],
      "source": [
        "#@title 5. Launch Gradio App with Public Share Link ğŸš€\n",
        "import os\n",
        "os.chdir(\"/content/pdf-translator-enhanced\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"   ğŸ“„ PDF Translator - Starting Gradio UI\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(\"ğŸŒ A PUBLIC SHARE LINK will be generated below!\")\n",
        "print(\"ğŸ“± You can access it from any device for 72 hours.\")\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run the app in share mode\n",
        "!python gradio_app.py --share"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
