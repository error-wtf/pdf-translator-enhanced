{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üìÑ PDF Translator Enhanced v2.0 - Colab Edition\n",
        "\n",
        "**Translate scientific PDFs with 100% formula preservation**\n",
        "\n",
        "> üîó **GitHub:** [error-wtf/pdf-translator-enhanced](https://github.com/error-wtf/pdf-translator-enhanced)  \n",
        "> Based on [thelanguagenerd/pdf-translator](https://github.com/thelanguagenerd/pdf-translator)\n",
        "\n",
        "¬© 2025 Sven Kalinowski with small help of Lino Casu  \n",
        "Licensed under the Anti-Capitalist Software License v1.4\n",
        "\n",
        "---\n",
        "\n",
        "## üÜï v2.0 Features\n",
        "\n",
        "- **üî¨ 100% Formula Preservation** - Hash-based isolation, no corruption\n",
        "- **üåç 25 Languages** - Including CJK, RTL (Arabic, Hebrew), Cyrillic\n",
        "- **üìä Quality Scoring** - Automatic 0-100 evaluation\n",
        "- **üìÑ Sequential Processing** - Handles PDFs of ANY size (page-by-page)\n",
        "- **üíæ Caching** - Faster re-translations\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ How It Works\n",
        "\n",
        "1. **Run all cells** - Installs Ollama, clones repo, installs dependencies\n",
        "2. **Gradio launches** with a **public share link**\n",
        "3. **Access from any device** - The link works for 72 hours\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Recommended Models\n",
        "\n",
        "| Model | Size | Quality | VRAM |\n",
        "|-------|------|---------|------|\n",
        "| **`qwen2.5:7b`** ‚≠ê | 4.4 GB | Excellent | 8 GB |\n",
        "| `llama3.1:8b` | 4.7 GB | Very Good | 10 GB |\n",
        "| `mistral:7b` | 4.1 GB | Very Good | 8 GB |\n",
        "| `llama3.2:3b` | 2 GB | Good | 4 GB |\n",
        "\n",
        "---\n",
        "\n",
        "## GPU Runtime Setup\n",
        "\n",
        "1. Click **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **T4 GPU** (free) or **A100** (Colab Pro)\n",
        "3. Click **Save**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "#@title 1. Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "import torch\n",
        "print(f\"\\n‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"üéÆ GPU: {gpu_name}\")\n",
        "    print(f\"üíæ VRAM: {vram_gb:.1f} GB\")\n",
        "    if vram_gb >= 16:\n",
        "        print(\"\\nüìå Recommended: qwen2.5:7b or llama3.1:8b\")\n",
        "    elif vram_gb >= 8:\n",
        "        print(\"\\nüìå Recommended: mistral:7b or llama3.2:3b\")\n",
        "    else:\n",
        "        print(\"\\nüìå Recommended: llama3.2:3b\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No GPU detected! Please enable GPU runtime:\")\n",
        "    print(\"   Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 2. Install Ollama\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "print(\"\\n‚úÖ Ollama installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 3. Start Ollama & Download Model\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if running\n",
        "try:\n",
        "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "    if r.status_code == 200:\n",
        "        print(\"‚úÖ Ollama server running!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Ollama not responding. Run this cell again.\")\n",
        "\n",
        "# Download model\n",
        "model = \"qwen2.5:7b\"  #@param [\"qwen2.5:7b\", \"llama3.1:8b\", \"mistral:7b\", \"llama3.2:3b\", \"openchat:7b\"]\n",
        "print(f\"\\nüì• Downloading {model} (this may take a few minutes)...\")\n",
        "!ollama pull {model}\n",
        "print(f\"\\n‚úÖ Model {model} ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "#@title 4. Clone Repository & Install Dependencies\n",
        "%cd /content\n",
        "!rm -rf pdf-translator-enhanced 2>/dev/null\n",
        "!git clone https://github.com/error-wtf/pdf-translator-enhanced.git\n",
        "%cd pdf-translator-enhanced\n",
        "\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"\\n‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_gradio"
      },
      "outputs": [],
      "source": [
        "#@title 5. Launch Gradio App with Public Share Link üöÄ\n",
        "import os\n",
        "os.chdir(\"/content/pdf-translator-enhanced\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"   üìÑ PDF Translator v2.0 - Starting Gradio UI\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(\"üåê A PUBLIC SHARE LINK will be generated below!\")\n",
        "print(\"üì± You can access it from any device for 72 hours.\")\n",
        "print(\"\")\n",
        "print(\"‚ú® v2.0 Features:\")\n",
        "print(\"   - 100% Formula Preservation\")\n",
        "print(\"   - 25 Languages (including CJK, RTL)\")\n",
        "print(\"   - Sequential Processing (handles ANY PDF size)\")\n",
        "print(\"   - Quality Scoring (0-100)\")\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run the app with --share argument\n",
        "!python gradio_app.py --share"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
