{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üìÑ PDF Translator - Full Gradio Version\n",
        "\n",
        "**Translate scientific PDFs with the complete Gradio UI**\n",
        "\n",
        "> üîó **GitHub:** [error-wtf/pdf-translator-enhanced](https://github.com/error-wtf/pdf-translator-enhanced)  \n",
        "> Based on [thelanguagenerd/pdf-translator](https://github.com/thelanguagenerd/pdf-translator)\n",
        "\n",
        "¬© 2025 Sven Kalinowski with small help of Lino Casu  \n",
        "Licensed under the Anti-Capitalist Software License v1.4\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ How It Works\n",
        "\n",
        "1. **Run all cells** - Installs Ollama, clones repo, installs dependencies\n",
        "2. **Gradio launches** with a **public share link**\n",
        "3. **Access from any device** - The link works for 72 hours\n",
        "4. **Full features** - All 20 languages, formula protection, table detection\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Features\n",
        "\n",
        "- **üî¨ Formula Protection** - 60+ LaTeX patterns protected during translation\n",
        "- **üåç 20 Languages** - German, English, Arabic, Hebrew, Ukrainian, Japanese, Chinese, Hindi...\n",
        "- **üìä Table Detection** - Preserves table structure\n",
        "- **üñºÔ∏è Caption Anchoring** - Figures stay with captions\n",
        "- **üîí 100% Local** - With Ollama local models, no data leaves Google's servers\n",
        "- **‚òÅÔ∏è Optional Ollama Cloud** - Use `...-cloud` models with internet + `ollama signin`\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Recommended Models\n",
        "\n",
        "| Model | Size | Description | VRAM |\n",
        "|-------|------|-------------|------|\n",
        "| **`openchat:7b`** ‚≠ê | 4.1 GB | ChatGPT-like, great for translations | 8 GB |\n",
        "| `qwen2.5:7b` | 4.4 GB | Excellent multilingual | 8 GB |\n",
        "| `llama3.1:8b` | 4.7 GB | Meta's latest | 10 GB |\n",
        "| `neural-chat:7b` | 4.1 GB | Intel optimized | 8 GB |\n",
        "| `qwen2.5:14b` | 9 GB | Best quality | 16 GB |\n",
        "\n",
        "---\n",
        "\n",
        "## GPU Runtime Setup\n",
        "\n",
        "1. Click **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **T4 GPU** (free) or **A100** (Colab Pro)\n",
        "3. Click **Save**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "#@title 1. Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "import torch\n",
        "print(f\"\\n‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"üéÆ GPU: {gpu_name}\")\n",
        "    print(f\"üíæ VRAM: {vram_gb:.1f} GB\")\n",
        "    if vram_gb >= 24:\n",
        "        print(\"\\nüìå Recommended: qwen2.5:14b or mistral-nemo:12b\")\n",
        "    elif vram_gb >= 16:\n",
        "        print(\"\\nüìå Recommended: llama3.1:8b or qwen2.5:7b\")\n",
        "    elif vram_gb >= 8:\n",
        "        print(\"\\nüìå Recommended: llama3.2:3b or phi3:mini\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Low VRAM - use OpenAI backend or smaller models\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No GPU detected! Please enable GPU runtime:\")\n",
        "    print(\"   Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 2. Install Ollama\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "print(\"\\n‚úÖ Ollama installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 3. Start Ollama & Download Model\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if running\n",
        "try:\n",
        "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "    if r.status_code == 200:\n",
        "        print(\"‚úÖ Ollama server running!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Ollama not responding. Run this cell again.\")\n",
        "\n",
        "# Download model\n",
        "# openchat:7b is a ChatGPT-like open source model, great for translations\n",
        "model = \"openchat:7b\"  #@param [\"openchat:7b\", \"gpt-oss:20b\", \"gpt-oss:20b-cloud\", \"gpt-oss:120b-cloud\", \"deepseek-v3.1:671b-cloud\", \"qwen3-coder:480b-cloud\", \"qwen2.5:7b\", \"llama3.1:8b\", \"qwen2.5:14b\", \"mistral:7b\", \"mistral-nemo:12b\", \"llama3.2:3b\", \"neural-chat:7b\"]\n",
        "print(f\"\\nüì• Downloading {model} (this may take a few minutes)...\")\n",
        "print(\"üí° openchat:7b is a ChatGPT-like open source model - great for translations!\")\n",
        "if model.endswith(\"-cloud\"):\n",
        "    print(\"\\n‚òÅÔ∏è Cloud model selected. Requires internet + Ollama v0.12+ and you must be signed in:\")\n",
        "    print(\"   ollama signin\")\n",
        "\n",
        "!ollama pull {model}\n",
        "print(f\"\\n‚úÖ Model {model} ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "#@title 4. Clone Repository & Install Dependencies\n",
        "%cd /content\n",
        "!rm -rf pdf-translator-enhanced 2>/dev/null\n",
        "!git clone https://github.com/error-wtf/pdf-translator-enhanced.git\n",
        "%cd pdf-translator-enhanced\n",
        "\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"\\n‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_gradio"
      },
      "outputs": [],
      "source": [
        "#@title 5. Launch Gradio App with Public Share Link üöÄ\n",
        "import os\n",
        "os.chdir(\"/content/pdf-translator-enhanced\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"   üìÑ PDF Translator - Starting Gradio UI\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(\"üåê A PUBLIC SHARE LINK will be generated below!\")\n",
        "print(\"üì± You can access it from any device for 72 hours.\")\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Patch gradio_app.py to use share=True\n",
        "with open(\"gradio_app.py\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Replace launch() to include share=True\n",
        "if \"share=True\" not in content:\n",
        "    content = content.replace(\n",
        "        \"demo.launch()\",\n",
        "        \"demo.launch(share=True)\"\n",
        "    )\n",
        "    content = content.replace(\n",
        "        \"demo.launch(server_name=\",\n",
        "        \"demo.launch(share=True, server_name=\"\n",
        "    )\n",
        "    with open(\"gradio_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "# Run the app\n",
        "!python gradio_app.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
