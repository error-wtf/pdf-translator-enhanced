{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üìÑ PDF Translator Enhanced v2.0 - Colab Edition\n",
        "\n",
        "**Translate scientific PDFs with 100% formula preservation**\n",
        "\n",
        "> üîó **GitHub:** [error-wtf/pdf-translator-enhanced](https://github.com/error-wtf/pdf-translator-enhanced)  \n",
        "> Based on [thelanguagenerd/pdf-translator](https://github.com/thelanguagenerd/pdf-translator)\n",
        "\n",
        "¬© 2025 Sven Kalinowski with small help of Lino Casu  \n",
        "Licensed under the Anti-Capitalist Software License v1.4\n",
        "\n",
        "---\n",
        "\n",
        "## üÜï v2.0 Features\n",
        "\n",
        "- **üî¨ 100% Formula Preservation** - Hash-based isolation, no corruption\n",
        "- **üåç 25 Languages** - Including CJK, RTL (Arabic, Hebrew), Cyrillic\n",
        "- **üìä Quality Scoring** - Automatic 0-100 evaluation\n",
        "- **üìÑ Sequential Processing** - Handles PDFs of ANY size (page-by-page)\n",
        "- **üíæ Caching** - Faster re-translations\n",
        "- **‚òÅÔ∏è Cloud Models** - Use huge models without local GPU!\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ How It Works\n",
        "\n",
        "1. **Run all cells** - Installs Ollama, clones repo, installs dependencies\n",
        "2. **Gradio launches** with a **public share link**\n",
        "3. **Access from any device** - The link works for 72 hours\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Available Models\n",
        "\n",
        "### Local Models (runs on Colab GPU)\n",
        "| Model | Size | Quality | VRAM |\n",
        "|-------|------|---------|------|\n",
        "| `gemma2:2b` | 1.6 GB | Basic | 4 GB |\n",
        "| `phi3:mini` | 2.3 GB | Good | 4 GB |\n",
        "| `llama3.2:3b` | 2 GB | Good | 4 GB |\n",
        "| `mistral:7b` | 4.1 GB | Very Good | 8 GB |\n",
        "| **`qwen2.5:7b`** ‚≠ê | 4.4 GB | Excellent | 8 GB |\n",
        "| `llama3.1:8b` | 4.7 GB | Very Good | 10 GB |\n",
        "| `openchat:7b` | 4.1 GB | Very Good | 8 GB |\n",
        "| `qwen2.5:14b` | 9 GB | Premium | 16 GB |\n",
        "| `deepseek-coder-v2:16b` | 9 GB | Premium | 16 GB |\n",
        "| `gpt-oss:20b` | 12 GB | Premium | 16 GB |\n",
        "\n",
        "### ‚òÅÔ∏è Cloud Models (NO GPU needed!)\n",
        "| Model | Parameters | Quality |\n",
        "|-------|------------|----------|\n",
        "| `gpt-oss:20b-cloud` | 20B | Good |\n",
        "| `gpt-oss:120b-cloud` | 120B | Excellent |\n",
        "| `qwen2.5:32b-cloud` | 32B | Premium |\n",
        "| `qwen2.5:72b-cloud` | 72B | Maximum |\n",
        "| `deepseek-v3:671b-cloud` | 671B | Maximum |\n",
        "\n",
        "‚ö†Ô∏è Cloud models require: `ollama signin`\n",
        "\n",
        "---\n",
        "\n",
        "## GPU Runtime Setup\n",
        "\n",
        "1. Click **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **T4 GPU** (free) or **A100** (Colab Pro)\n",
        "3. Click **Save**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "#@title 1. Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "import torch\n",
        "print(f\"\\n‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"üéÆ GPU: {gpu_name}\")\n",
        "    print(f\"üíæ VRAM: {vram_gb:.1f} GB\")\n",
        "    if vram_gb >= 16:\n",
        "        print(\"\\nüìå Recommended: qwen2.5:14b, deepseek-coder-v2:16b, or gpt-oss:20b\")\n",
        "    elif vram_gb >= 8:\n",
        "        print(\"\\nüìå Recommended: qwen2.5:7b or mistral:7b\")\n",
        "    else:\n",
        "        print(\"\\nüìå Recommended: llama3.2:3b, phi3:mini, or gemma2:2b\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No GPU detected!\")\n",
        "    print(\"üìå Use ‚òÅÔ∏è Cloud models (no GPU needed):\")\n",
        "    print(\"   - gpt-oss:120b-cloud\")\n",
        "    print(\"   - deepseek-v3:671b-cloud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 2. Install Ollama\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "print(\"\\n‚úÖ Ollama installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "#@title 3. Start Ollama & Download Model\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if running\n",
        "try:\n",
        "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "    if r.status_code == 200:\n",
        "        print(\"‚úÖ Ollama server running!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Ollama not responding. Run this cell again.\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MODEL LIST\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# LOCAL (up to 16 GB VRAM - fits on Colab T4/A100):\n",
        "#   gemma2:2b, phi3:mini, llama3.2:3b, mistral:7b, qwen2.5:7b,\n",
        "#   llama3.1:8b, openchat:7b, qwen2.5:14b, deepseek-coder-v2:16b,\n",
        "#   gpt-oss:20b\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# CLOUD (no GPU needed, requires 'ollama signin'):\n",
        "#   gpt-oss:20b-cloud, gpt-oss:120b-cloud, qwen2.5:32b-cloud,\n",
        "#   qwen2.5:72b-cloud, deepseek-v3:671b-cloud\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "model = \"qwen2.5:7b\"  #@param [\"gemma2:2b\", \"phi3:mini\", \"llama3.2:3b\", \"mistral:7b\", \"qwen2.5:7b\", \"llama3.1:8b\", \"openchat:7b\", \"qwen2.5:14b\", \"deepseek-coder-v2:16b\", \"gpt-oss:20b\", \"gpt-oss:20b-cloud\", \"gpt-oss:120b-cloud\", \"qwen2.5:32b-cloud\", \"qwen2.5:72b-cloud\", \"deepseek-v3:671b-cloud\"]\n",
        "\n",
        "is_cloud = model.endswith(\"-cloud\")\n",
        "\n",
        "if is_cloud:\n",
        "    print(f\"\\n‚òÅÔ∏è Cloud model selected: {model}\")\n",
        "    print(\"\\n‚ö†Ô∏è Cloud models require Ollama signin:\")\n",
        "    print(\"   Run: ollama signin\")\n",
        "    print(\"   Then run this cell again.\")\n",
        "else:\n",
        "    # Show model size info\n",
        "    model_sizes = {\n",
        "        \"gemma2:2b\": \"1.6 GB\", \"phi3:mini\": \"2.3 GB\", \"llama3.2:3b\": \"2 GB\",\n",
        "        \"mistral:7b\": \"4.1 GB\", \"qwen2.5:7b\": \"4.4 GB\", \"llama3.1:8b\": \"4.7 GB\",\n",
        "        \"openchat:7b\": \"4.1 GB\", \"qwen2.5:14b\": \"9 GB\",\n",
        "        \"deepseek-coder-v2:16b\": \"9 GB\", \"gpt-oss:20b\": \"12 GB\"\n",
        "    }\n",
        "    size = model_sizes.get(model, \"unknown\")\n",
        "    print(f\"\\nüì• Downloading {model} ({size})...\")\n",
        "    print(\"   This may take a few minutes.\")\n",
        "\n",
        "!ollama pull {model}\n",
        "print(f\"\\n‚úÖ Model {model} ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "#@title 4. Clone Repository & Install Dependencies\n",
        "%cd /content\n",
        "!rm -rf pdf-translator-enhanced 2>/dev/null\n",
        "!git clone https://github.com/error-wtf/pdf-translator-enhanced.git\n",
        "%cd pdf-translator-enhanced\n",
        "\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"\\n‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_gradio"
      },
      "outputs": [],
      "source": [
        "#@title 5. Launch Gradio App with Public Share Link üöÄ\n",
        "import os\n",
        "os.chdir(\"/content/pdf-translator-enhanced\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"   üìÑ PDF Translator v2.0 - Starting Gradio UI\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(\"üåê A PUBLIC SHARE LINK will be generated below!\")\n",
        "print(\"üì± You can access it from any device for 72 hours.\")\n",
        "print(\"\")\n",
        "print(\"‚ú® v2.0 Features:\")\n",
        "print(\"   - 100% Formula Preservation\")\n",
        "print(\"   - 25 Languages (including CJK, RTL)\")\n",
        "print(\"   - Sequential Processing (handles ANY PDF size)\")\n",
        "print(\"   - Quality Scoring (0-100)\")\n",
        "print(\"   - Retry Logic for stable translations\")\n",
        "print(\"   - ETA & Progress Display\")\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run the app with --share argument\n",
        "!python gradio_app.py --share"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
